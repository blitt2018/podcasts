{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import random\n",
    "import seaborn as sns\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json(\"/shared/3/projects/benlitterer/podcastData/processed/floydMonth/floydMonthDataClean.jsonl\", orient=\"records\", lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#metaDf = pd.read_json(\"/shared/3/projects/benlitterer/podcastData/processed/mayJune/mayJuneMetadata.jsonl\", orient=\"record\", lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#if we don't already have entities for the transcripts and the descriptions, we need to add them... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#just in case \n",
    "inDf = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop_duplicates(subset=[\"potentialOutPath\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#do we want to work with podcasts where there's no description? or no transcript? let's say no for now\n",
    "df = df.dropna(subset=[\"rssUrl\", \"transcript\", \"enclosure\", \"potentialOutPath\", \"epDescription\", \"podDescription\",  \"category1\", \"transEnts\", \"descEnts\", \"epDescEnts\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#odd that we only have 366,013 left after removing na's. Seems to be driven mostly by ents\n",
    "#BUT: remember that many descriptions won't have entities  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#if either description or transcript has person entity and isn't none/na \n",
    "#then keep it\n",
    "def hasPerson(inRow):\n",
    "    #descTypes = inRow[\"DescType\"]\n",
    "    transTypes = inRow[\"transTypes\"]\n",
    "\n",
    "    #descHas = False\n",
    "    transHas = False\n",
    "\n",
    "    \"\"\"\n",
    "    if descTypes == descTypes and descTypes != None: \n",
    "        descHas = \"PERSON\" in descTypes\n",
    "    \"\"\"\n",
    "        \n",
    "    if transTypes == transTypes and transTypes != None: \n",
    "        transHas = \"PERSON\" in transTypes\n",
    "\n",
    "    return transHas\n",
    "\n",
    "\n",
    "#keep only those rows where we have a person in the description or the transcript\n",
    "#TODO: modify this\n",
    "df[\"hasPerson\"] = df.apply(hasPerson, axis=1)\n",
    "#df[\"hasPerson\"] = df[\"transEnts\"].apply(lambda x: x != None and x == x and len(x) > 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(214244, 77)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df[\"hasPerson\"] == True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique Categories: 26\n",
      "0       religion\n",
      "1       business\n",
      "2        society\n",
      "3      education\n",
      "4         sports\n",
      "5           news\n",
      "6         health\n",
      "7           arts\n",
      "8         comedy\n",
      "9             tv\n",
      "10       leisure\n",
      "11         music\n",
      "12    technology\n",
      "13          kids\n",
      "14       science\n",
      "15       fiction\n",
      "16       history\n",
      "17    government\n",
      "18    true crime\n",
      "19         games\n",
      "Name: category1, dtype: object\n"
     ]
    }
   ],
   "source": [
    "N_CATS=20\n",
    "print(f'Unique Categories: {len(df[\"category1\"].unique())}')\n",
    "topCats = pd.DataFrame(df[\"category1\"].value_counts()).reset_index().head(N_CATS)[\"category1\"]\n",
    "print(topCats)\n",
    "topCats = list(topCats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampDf = df[df[\"category1\"].apply(lambda x: x in topCats)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3021634/1225967707.py:3: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  sampDf = sampDf.groupby(by=\"category1\").apply(lambda x: x.sample(SAMP_NUM, random_state=SEED)).droplevel(\"category1\")\n"
     ]
    }
   ],
   "source": [
    "SAMP_NUM = 100\n",
    "SEED = 24\n",
    "sampDf = sampDf.groupby(by=\"category1\").apply(lambda x: x.sample(SAMP_NUM, random_state=SEED)).droplevel(\"category1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wrap(inStr): \n",
    "    i = 0 \n",
    "    STEP = 20\n",
    "    splitStr = inStr.split()\n",
    "    outStr = \"\"\n",
    "    while i < len(splitStr): \n",
    "        if i + STEP < len(splitStr):\n",
    "            outStr += \" \".join(splitStr[i:i+STEP]) + \"\\n\"\n",
    "        else: \n",
    "            outStr +=  \" \".join(splitStr[i:]) + \"\\n\"\n",
    "        i += STEP\n",
    "    print(outStr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.head()[\"transcript\"]\n",
    "#did some checks on data quality and things look better after 4-gram cleaning\n",
    "#print(wrap(df.sample()[\"transcript\"].item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaner(inStr, LEN): \n",
    "    #clean out all tags in brackets or parenthesis\n",
    "    inStr = re.sub(r'\\[[^()]*\\]\\s*', '', inStr)\n",
    "    inStr = re.sub(r'\\([^()]*\\)\\s*', '', inStr)\n",
    "    \n",
    "    #replace paragraph breaks with new lines  \n",
    "    inStr = re.sub(\"</p>\",\". \", inStr)\n",
    "    inStr = re.sub(\"</br>\", \". \", inStr)\n",
    "\n",
    "    #replace all other tags with space\n",
    "    inStr = re.sub(\"<.+?>\", \" \", inStr)\n",
    "\n",
    "    #replace all amounts of space with a single space \n",
    "    inStr = re.sub(r\"[ \\t]+\", \" \", inStr)\n",
    "    \n",
    "    inStr = BeautifulSoup(inStr, \"html.parser\").text\n",
    "    if len(inStr.split()) > LEN: \n",
    "        inStr = \" \".join(inStr.split()[:LEN]) + \"...\"\n",
    "    \n",
    "    return inStr\n",
    "\n",
    "#TODO: clean out html tags and stuff like that...\n",
    "#TODO: just replace tags with a space and make spaces and then make one space between every word  \n",
    "\n",
    "#f\"<strong>Description:</strong><br>{description}<br><br><strong>Transcript:</strong><br>{transcript}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3021634/3321707436.py:16: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  inStr = BeautifulSoup(inStr, \"html.parser\").text\n",
      "/tmp/ipykernel_3021634/3321707436.py:16: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  inStr = BeautifulSoup(inStr, \"html.parser\").text\n"
     ]
    }
   ],
   "source": [
    "sampDf[\"descClean\"] = sampDf[\"podDescription\"].apply(cleaner, args=[150])\n",
    "sampDf[\"epDescClean\"] =  sampDf[\"epDescription\"].apply(cleaner, args=[150])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "#subset the columns we're working with \n",
    "sampDf = sampDf[[\"potentialOutPath\", \"descClean\", \"epDescClean\", \"transcript\", \"transEnts\", \"transTypes\",  \"rssUrl\", \"enclosure\", \"cleanDatesLoc\", \"epTitle\", \"title\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampDf = sampDf.explode([\"transTypes\", \"transEnts\"])\n",
    "sampDf = sampDf[sampDf[\"transTypes\"] == \"PERSON\"]\n",
    "sampDf = sampDf.dropna(subset=[\"transEnts\"])\n",
    "sampDf = sampDf[sampDf[\"transEnts\"].apply(lambda x: len(x.split())) == 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "#only get entities where the first word starts with a capital letter \n",
    "sampDf = sampDf[sampDf[\"transEnts\"].apply(lambda x: x[0].isupper())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(29081, 11)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#remove any honorifics - entities with [Mr., Mrs., Dr., Professor, Prof., ]\n",
    "#CHECKING: sampDf[sampDf[\"transEnts\"].str.contains(\"\\\\.\")].sample(10)\n",
    "def hasHonorific(inStr): \n",
    "    honorifics = [\"Mr \", \"Mrs \", \"Dr \", \"Doctor \", \"Professor\", \"Prof.\"]\n",
    "    for honorific in honorifics: \n",
    "        if honorific in inStr: \n",
    "            return True\n",
    "    return False \n",
    "\n",
    "sampDf = sampDf[sampDf[\"transEnts\"].apply(hasHonorific) == False]\n",
    "sampDf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: add podcast title here \n",
    "#crop the transcript so we can ensure we get \n",
    "def getCroppedTranscript(inRow, BUFFER=150): \n",
    "    ent = inRow[\"transEnts\"]\n",
    "    trans = inRow[\"transcript\"]\n",
    "\n",
    "    #we can assume single space between every token now? \n",
    "    trans = \" \".join(trans.split())\n",
    "\n",
    "    #trans = re.sub(r\"[^a-zA-Z ]\", \"\", trans)\n",
    "    #ent = re.sub(r\"[^a-zA-Z '.\\-]\", \"\", ent)\n",
    "    ent = re.sub(r\"\\\\\", \"\", ent)\n",
    "    ent = \" \".join(ent.split())\n",
    "    #print(ent)\n",
    "    \n",
    "    #ent = re.sub(r\"[^a-zA-Z ]\", \"\", ent)\n",
    "    #find where the entity occurs\n",
    "    match = re.search(ent, trans, re.IGNORECASE)\n",
    "\n",
    "    if match == None or len(ent) == 0: \n",
    "        return \"NO_MATCH\"\n",
    "\n",
    "    start = match.start() \n",
    "    end = match.end() \n",
    "\n",
    "    #get 200 words in either direction of our match \n",
    "    transList = trans.split()\n",
    "\n",
    "    \n",
    "    #get index for where our entity starts  \n",
    "    charIndex = 0 \n",
    "    entIndex = len(transList)\n",
    "    tokIndex = 0\n",
    "    while tokIndex < len(transList):\n",
    "        tok = transList[tokIndex]\n",
    "        #the moment our char index passes the char index \n",
    "        #of the beginning of our token, we nominate the current token index as\n",
    "        #the first one of our entity \n",
    "        if charIndex >= start:  \n",
    "            entIndex = tokIndex \n",
    "            break \n",
    "\n",
    "        #move char index ahead accounting for space inbetween words\n",
    "        charIndex += len(tok)\n",
    "        charIndex += 1         \n",
    "        tokIndex += 1\n",
    "\n",
    "   \n",
    "    #we only want entities towards the beginning of the transcript \n",
    "    if entIndex > 350: \n",
    "        return \"TOO_FAR\"\n",
    "\n",
    "\n",
    "    #ensure we won't start before or after the end of the transcript list \n",
    "    start = max(entIndex - BUFFER, 0)\n",
    "    end = min(entIndex + len(ent.split()) + 1 + BUFFER, len(transList))\n",
    "\n",
    "    transcriptChunk = \" \".join(transList[start:end])\n",
    "\n",
    "    return transcriptChunk\n",
    "\n",
    "#I confirmed that we are actually finding the entity, just not early in the transcript!\n",
    "sampDf[\"transChunk\"] = sampDf.apply(getCroppedTranscript, axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deviant entity matches: 2\n",
      "prop. deviant entity matches: 6.87734259482136e-05\n",
      "entities too far along in transcript:22854\n",
      "prop. entities too far along in transcript:0.7858739383102369\n"
     ]
    }
   ],
   "source": [
    "deviantRows = len(sampDf[(sampDf[\"transChunk\"] == \"NO_MATCH\") | (sampDf[\"transChunk\"] == \"NOT_IN_LIST\")])\n",
    "print(f\"deviant entity matches: {deviantRows}\")\n",
    "print(f\"prop. deviant entity matches: {deviantRows/len(sampDf)}\")\n",
    "tooFar = len(sampDf[sampDf[\"transChunk\"] == \"TOO_FAR\"])\n",
    "print(f\"entities too far along in transcript:{tooFar}\")\n",
    "print(f\"prop. entities too far along in transcript:{tooFar/len(sampDf)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampDf = sampDf[(sampDf[\"transChunk\"] != \"TOO_FAR\") & (sampDf[\"transChunk\"] != \"NO_MATCH\") & (sampDf[\"transChunk\"] != \"NOT_IN_LIST\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "72260    PK 203: Understanding Artistic Identity with Z...\n",
       "72260    PK 203: Understanding Artistic Identity with Z...\n",
       "47612                       Episode 155: Excellent Bullets\n",
       "47612                       Episode 155: Excellent Bullets\n",
       "79027    From Human-Centered Design to Relationship-Cen...\n",
       "Name: epTitle, dtype: object"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampDf.head()[\"epTitle\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#fix tiny misspellings \n",
    "def getDisplayText(inRow): \n",
    "    ent = inRow[\"transEnts\"]\n",
    "\n",
    "    podTitle = inRow[\"title\"]\n",
    "    epTitle = inRow[\"epTitle\"]\n",
    "    desc = inRow[\"descClean\"] #if inRow[\"descClean\"] == inRow[\"descClean\"] and inRow[\"descClean\"] != None else \"\" \n",
    "    epDesc = inRow[\"epDescClean\"] #if inRow[\"epDescClean\"] == inRow[\"epDescClean\"] and inRow[\"epDescClean\"] != None else \"\" \n",
    "    trans = inRow[\"transChunk\"] #if inRow[\"transcript\"] == inRow[\"transcript\"] and inRow[\"transcript\"] != None else \"\" \n",
    "    fullTrans = inRow[\"transcript\"]\n",
    "\n",
    "    #add ellipses if we've modified the transcript in any way \n",
    "    fullTrans = \" \".join(fullTrans.split())\n",
    "    if trans[:3] != fullTrans[:3]: \n",
    "        trans = \"...\" + trans\n",
    "    \n",
    "    allText = \"<strong>Podcast Title: </strong>\" + podTitle + \"<br><strong>Podcast Description:</strong><br>\" + \\\n",
    "    desc + \"<br><br><strong>Podcast Episode Title: </strong>\" + epTitle + \"<br><strong>Podcast Episode Description:</strong><br>\" + epDesc \\\n",
    "    + \"<br><br><strong>Podcast Transcript Excerpt:</strong><br>\" + trans\n",
    "\n",
    "    #remove html tags, as they mess up the formatting\n",
    "    #also remove any extraneous spaces \n",
    "    allText = re.sub(r\"http[^\\s<]*\", \"[hyperlink]\", allText)\n",
    "    allText = \" \".join(allText.split())\n",
    "    #allText = allText.lower()\n",
    "\n",
    "    #clean up the entity a bit here \n",
    "    #single space in-between should align with our cleaning in text above \n",
    "    ent = re.sub(r\"\\\\\", \"\", ent)\n",
    "    ent = ent.replace(\".\", \"\\.\").replace(\"*\", \"\\*\").replace(\"+\", \"\\+\").replace(\"?\", \"\\?\")\n",
    "    ent = \" \".join(ent.split())\n",
    "\n",
    "    #print(ent)\n",
    "    #bold full occurences of the entity \n",
    "    #note that we match irrespective of case \n",
    "    outText = re.sub(ent, f'<span style=\"background-color:#00FF00\">{ent}</span>', allText, flags=re.IGNORECASE)\n",
    "    outText = outText.replace(\"<a href=\", \"\")\n",
    "    \n",
    "    #if we have a multi-word entity, highlight the first word where it hasn't already been bolded \n",
    "    firstEnt = ent.split()[0]\n",
    "    if len(ent.split()) > 1: \n",
    "        outText = re.sub(r\"(?<!>)\" + firstEnt, f'<span style=\"background-color:#00FF00\">{firstEnt}</span>', outText, flags=re.IGNORECASE)\n",
    "\n",
    "    outText = f\"<span style='background-color:#00FF00'>Target Entity: {ent}</span><br><br>{outText}\"\n",
    "    return outText\n",
    "    #return [[item.start(), item.end(), ent] for item in re.finditer(ent, allText)]\n",
    "\n",
    "sampDf[\"displayText\"] = list(sampDf.apply(getDisplayText, axis=1))\n",
    "#sampDf.head().apply(getEntIx, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>potentialOutPath</th>\n",
       "      <th>descClean</th>\n",
       "      <th>epDescClean</th>\n",
       "      <th>transcript</th>\n",
       "      <th>transEnts</th>\n",
       "      <th>transTypes</th>\n",
       "      <th>rssUrl</th>\n",
       "      <th>enclosure</th>\n",
       "      <th>cleanDatesLoc</th>\n",
       "      <th>epTitle</th>\n",
       "      <th>title</th>\n",
       "      <th>transChunk</th>\n",
       "      <th>displayText</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>72260</th>\n",
       "      <td>/traffic.libsyn.com/ni/httpstraffic.libsyn.com...</td>\n",
       "      <td>The Pencil Kings Podcast interviews today’s to...</td>\n",
       "      <td>Zhiwan Cheung is a Fulbright Fellow, artist, v...</td>\n",
       "      <td>[MUSIC] Welcome to the Pencil King show, where...</td>\n",
       "      <td>Mitch Voller</td>\n",
       "      <td>PERSON</td>\n",
       "      <td>https://pencilkings.libsyn.com/rss</td>\n",
       "      <td>https://traffic.libsyn.com/secure/pencilkings/...</td>\n",
       "      <td>2020-05-20 06:00:00+00:00</td>\n",
       "      <td>PK 203: Understanding Artistic Identity with Z...</td>\n",
       "      <td>Pencil Kings | Inspiring Artist Interviews wit...</td>\n",
       "      <td>[MUSIC] Welcome to the Pencil King show, where...</td>\n",
       "      <td>&lt;span style='background-color:#00FF00'&gt;Target ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72260</th>\n",
       "      <td>/traffic.libsyn.com/ni/httpstraffic.libsyn.com...</td>\n",
       "      <td>The Pencil Kings Podcast interviews today’s to...</td>\n",
       "      <td>Zhiwan Cheung is a Fulbright Fellow, artist, v...</td>\n",
       "      <td>[MUSIC] Welcome to the Pencil King show, where...</td>\n",
       "      <td>Zewon Chung</td>\n",
       "      <td>PERSON</td>\n",
       "      <td>https://pencilkings.libsyn.com/rss</td>\n",
       "      <td>https://traffic.libsyn.com/secure/pencilkings/...</td>\n",
       "      <td>2020-05-20 06:00:00+00:00</td>\n",
       "      <td>PK 203: Understanding Artistic Identity with Z...</td>\n",
       "      <td>Pencil Kings | Inspiring Artist Interviews wit...</td>\n",
       "      <td>take you there and beyond for fraction of the ...</td>\n",
       "      <td>&lt;span style='background-color:#00FF00'&gt;Target ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47612</th>\n",
       "      <td>/sphinx.acast.com/es/httpssphinx.acast.comseri...</td>\n",
       "      <td>A weekly comic book and manga review podcast. ...</td>\n",
       "      <td>Levins has gone JUNJI ITO CRAZY and Siobhan ha...</td>\n",
       "      <td>Hey Dave, since we founded Bombas, we've alway...</td>\n",
       "      <td>Andrew Levens</td>\n",
       "      <td>PERSON</td>\n",
       "      <td>https://rss.acast.com/serious-issues</td>\n",
       "      <td>https://sphinx.acast.com/serious-issues/episod...</td>\n",
       "      <td>2020-05-25 03:47:20+00:00</td>\n",
       "      <td>Episode 155: Excellent Bullets</td>\n",
       "      <td>Serious Issues: A Comic Book Podcast with Andr...</td>\n",
       "      <td>one purchase equals one donated. Wow, did we j...</td>\n",
       "      <td>&lt;span style='background-color:#00FF00'&gt;Target ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47612</th>\n",
       "      <td>/sphinx.acast.com/es/httpssphinx.acast.comseri...</td>\n",
       "      <td>A weekly comic book and manga review podcast. ...</td>\n",
       "      <td>Levins has gone JUNJI ITO CRAZY and Siobhan ha...</td>\n",
       "      <td>Hey Dave, since we founded Bombas, we've alway...</td>\n",
       "      <td>Chabon Coons</td>\n",
       "      <td>PERSON</td>\n",
       "      <td>https://rss.acast.com/serious-issues</td>\n",
       "      <td>https://sphinx.acast.com/serious-issues/episod...</td>\n",
       "      <td>2020-05-25 03:47:20+00:00</td>\n",
       "      <td>Episode 155: Excellent Bullets</td>\n",
       "      <td>Serious Issues: A Comic Book Podcast with Andr...</td>\n",
       "      <td>Wow, did we just write an ad? Yes. Bombas. Big...</td>\n",
       "      <td>&lt;span style='background-color:#00FF00'&gt;Target ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79027</th>\n",
       "      <td>/mcdn.podbean.com/en/httpsmcdn.podbean.commfwe...</td>\n",
       "      <td>Design is Everywhere features stories of peopl...</td>\n",
       "      <td>Many of us are aware of human-centered design,...</td>\n",
       "      <td>[MUSIC PLAYING] Hello, and welcome to Design i...</td>\n",
       "      <td>Sam Aquilano</td>\n",
       "      <td>PERSON</td>\n",
       "      <td>https://feed.podbean.com/designmuseum/feed.xml</td>\n",
       "      <td>https://mcdn.podbean.com/mf/web/7pqn9m/POD-007...</td>\n",
       "      <td>2020-06-04 10:00:00+00:00</td>\n",
       "      <td>From Human-Centered Design to Relationship-Cen...</td>\n",
       "      <td>Design is Everywhere</td>\n",
       "      <td>[MUSIC PLAYING] Hello, and welcome to Design i...</td>\n",
       "      <td>&lt;span style='background-color:#00FF00'&gt;Target ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        potentialOutPath  \\\n",
       "72260  /traffic.libsyn.com/ni/httpstraffic.libsyn.com...   \n",
       "72260  /traffic.libsyn.com/ni/httpstraffic.libsyn.com...   \n",
       "47612  /sphinx.acast.com/es/httpssphinx.acast.comseri...   \n",
       "47612  /sphinx.acast.com/es/httpssphinx.acast.comseri...   \n",
       "79027  /mcdn.podbean.com/en/httpsmcdn.podbean.commfwe...   \n",
       "\n",
       "                                               descClean  \\\n",
       "72260  The Pencil Kings Podcast interviews today’s to...   \n",
       "72260  The Pencil Kings Podcast interviews today’s to...   \n",
       "47612  A weekly comic book and manga review podcast. ...   \n",
       "47612  A weekly comic book and manga review podcast. ...   \n",
       "79027  Design is Everywhere features stories of peopl...   \n",
       "\n",
       "                                             epDescClean  \\\n",
       "72260  Zhiwan Cheung is a Fulbright Fellow, artist, v...   \n",
       "72260  Zhiwan Cheung is a Fulbright Fellow, artist, v...   \n",
       "47612  Levins has gone JUNJI ITO CRAZY and Siobhan ha...   \n",
       "47612  Levins has gone JUNJI ITO CRAZY and Siobhan ha...   \n",
       "79027  Many of us are aware of human-centered design,...   \n",
       "\n",
       "                                              transcript      transEnts  \\\n",
       "72260  [MUSIC] Welcome to the Pencil King show, where...   Mitch Voller   \n",
       "72260  [MUSIC] Welcome to the Pencil King show, where...    Zewon Chung   \n",
       "47612  Hey Dave, since we founded Bombas, we've alway...  Andrew Levens   \n",
       "47612  Hey Dave, since we founded Bombas, we've alway...   Chabon Coons   \n",
       "79027  [MUSIC PLAYING] Hello, and welcome to Design i...   Sam Aquilano   \n",
       "\n",
       "      transTypes                                          rssUrl  \\\n",
       "72260     PERSON              https://pencilkings.libsyn.com/rss   \n",
       "72260     PERSON              https://pencilkings.libsyn.com/rss   \n",
       "47612     PERSON            https://rss.acast.com/serious-issues   \n",
       "47612     PERSON            https://rss.acast.com/serious-issues   \n",
       "79027     PERSON  https://feed.podbean.com/designmuseum/feed.xml   \n",
       "\n",
       "                                               enclosure  \\\n",
       "72260  https://traffic.libsyn.com/secure/pencilkings/...   \n",
       "72260  https://traffic.libsyn.com/secure/pencilkings/...   \n",
       "47612  https://sphinx.acast.com/serious-issues/episod...   \n",
       "47612  https://sphinx.acast.com/serious-issues/episod...   \n",
       "79027  https://mcdn.podbean.com/mf/web/7pqn9m/POD-007...   \n",
       "\n",
       "                   cleanDatesLoc  \\\n",
       "72260  2020-05-20 06:00:00+00:00   \n",
       "72260  2020-05-20 06:00:00+00:00   \n",
       "47612  2020-05-25 03:47:20+00:00   \n",
       "47612  2020-05-25 03:47:20+00:00   \n",
       "79027  2020-06-04 10:00:00+00:00   \n",
       "\n",
       "                                                 epTitle  \\\n",
       "72260  PK 203: Understanding Artistic Identity with Z...   \n",
       "72260  PK 203: Understanding Artistic Identity with Z...   \n",
       "47612                     Episode 155: Excellent Bullets   \n",
       "47612                     Episode 155: Excellent Bullets   \n",
       "79027  From Human-Centered Design to Relationship-Cen...   \n",
       "\n",
       "                                                   title  \\\n",
       "72260  Pencil Kings | Inspiring Artist Interviews wit...   \n",
       "72260  Pencil Kings | Inspiring Artist Interviews wit...   \n",
       "47612  Serious Issues: A Comic Book Podcast with Andr...   \n",
       "47612  Serious Issues: A Comic Book Podcast with Andr...   \n",
       "79027                               Design is Everywhere   \n",
       "\n",
       "                                              transChunk  \\\n",
       "72260  [MUSIC] Welcome to the Pencil King show, where...   \n",
       "72260  take you there and beyond for fraction of the ...   \n",
       "47612  one purchase equals one donated. Wow, did we j...   \n",
       "47612  Wow, did we just write an ad? Yes. Bombas. Big...   \n",
       "79027  [MUSIC PLAYING] Hello, and welcome to Design i...   \n",
       "\n",
       "                                             displayText  \n",
       "72260  <span style='background-color:#00FF00'>Target ...  \n",
       "72260  <span style='background-color:#00FF00'>Target ...  \n",
       "47612  <span style='background-color:#00FF00'>Target ...  \n",
       "47612  <span style='background-color:#00FF00'>Target ...  \n",
       "79027  <span style='background-color:#00FF00'>Target ...  "
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampDf.head() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sampDf = sampDf.drop(columns=[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampDf[\"cleanEnt\"] = sampDf[\"transEnts\"].apply(lambda x: re.sub(r\"[^a-zA-Z ]\" , \"\", x.lower()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "#shuffle rows, remove duplicates on the cleaned version of our entities\n",
    "#then get one row per show \n",
    "sampDf = sampDf.sample(len(sampDf), random_state=SEED).drop_duplicates(\"cleanEnt\").drop_duplicates(\"rssUrl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape after dropping duplicates on entity and podcast: (1344, 14)\n"
     ]
    }
   ],
   "source": [
    "print(f\"shape after dropping duplicates on entity and podcast: {sampDf.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<span style='background-color:#00FF00'>Target Entity: David Politis</span><br><br><strong>Podcast Title: </strong>Four Corners Crime Cast<br><strong>Podcast Description:</strong><br>Four Corners Crime Cast brings you true crime from the\n",
      "four corner states: Arizona, Colorado, New Mexico, and Utah. Hosted by Katie Renner and cohosted by Jake Sanders and Rory\n",
      "Allard, we deep dive into killer's crimes and minds, with a little comedic relief in between.<br><br><strong>Podcast Episode Title: </strong>Episode 46:\n",
      "Missing Persons of Rocky Mountain National Park<br><strong>Podcast Episode Description:</strong><br> This week we are taking a little break from horrific awfulness,\n",
      "and covering missing persons of Rocky Mountain National Park. Join us as Katie takes us through some of the known\n",
      "cases, and the guys try, not very well, to help shed light on the multiple possibilities.. <br><br><strong>Podcast Transcript Excerpt:</strong><br>...favorite book\n",
      "I've ever read., \"Death in Rocky Mountain National Park, Accidents in Full Hardiness on the Continental Divide\"\" by Randy Meinter.\n",
      "And this book has two covers, right? Just to fit the title on it? Mm-hmm., \"Full Hardiness,.\"\" \"Full Hardiness,.\" It's\n",
      "about people that were either murdered or died in the half-a-linches or win-missing. That's all full hardiness. So I don't\n",
      "know where the full hardiness comes in the play, but... Isn't full hardiness like you're like rough housing on the\n",
      "edge of a cliff and you fall over type of thing? That full hardiness. I guess it could be considered\n",
      "full hardiness. I'm not sure which part. He didn't specify which part of the book he considered the full hardiness.\n",
      "No, do we have any other sources on this one Katie? Yeah, I used some old newspaper clippings that I\n",
      "found on the coloradoin.com And unfortunately snippets of <span style=\"background-color:#00FF00\">David Politis</span>' presentation given to Mufon LA, February 19, 2013, which\n",
      "I found on the UFO and Paranormal Research Society's YouTube channel. It was titled, \"Big Foot DNA and Missing 411,.\"\n",
      "Okay, so here's the thing about Big Foot is that potentially there could be a bigfoot. There could be a\n",
      "giant, cryptid society with enhanced magical power known as the forest people that live in the woods in places like\n",
      "the Colorado Rockies or in the Grand Tetons or up in the North Pacific Northwest near Mount Hood or anything\n",
      "like that. They can just live there without being trampled or seen or anything like that on zip lines. Yeah,\n",
      "but do you think <span style=\"background-color:#00FF00\">David Politis</span> was really the first one to find DNA for them? No, I don't.\n",
      "I mean, there's potential that that could be real. I didn't say that it was, but... Wasn't he the first\n",
      "guy to get\n",
      "\n"
     ]
    }
   ],
   "source": [
    "wrap(sampDf[\"displayText\"].iloc[900])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampDf = sampDf.rename(columns={\"displayText\":\"text\", \"enclosure\":\"id\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "#WHERE WERE GOING TO OUTPUT FOR NOW\n",
    "sampDf.to_json(\"/shared/3/projects/benlitterer/podcastData/annotation/label1000/2kpodsClassification.jsonl\", orient=\"records\", lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

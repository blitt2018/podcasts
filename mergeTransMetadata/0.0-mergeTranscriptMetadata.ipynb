{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "a5123576",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pytz\n",
    "import time\n",
    "import csv\n",
    "import importlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4f86cc55",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2867304/1307880365.py:7: DtypeWarning: Columns (5,6,7,11) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  metaDf = pd.read_csv(META_PATH, names=colNames).reset_index(names=[\"rssUrl\"])\n"
     ]
    }
   ],
   "source": [
    "#load in the metadata dataframe \n",
    "colNames = [\"epTitle\", \"epDescription\", \"duration\", \"pubDate\", \"copyright\", \"itunes:type\", \n",
    "           \"itunes:complete\", \"guid\", \"itunes:explicit\", \"enclosure\",\"itunes:image\", \"transDict\"]\n",
    "\n",
    "#this *appears* to be loading correctly \n",
    "META_PATH= \"/shared/3/projects/benlitterer/podcastData/podRss/floydFeeds/floydMetadata.csv\"\n",
    "metaDf = pd.read_csv(META_PATH, names=colNames).reset_index(names=[\"rssUrl\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c6e3e1d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2867304/174181308.py:2: DtypeWarning: Columns (1,8,9,14,24,25,38) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  podDf = pd.read_csv(POD_PATH, index_col=0).drop(columns=[\"Unnamed: 0\"]).rename(columns={\"url\":\"rssUrl\", \"description\":\"podDescription\"})\n"
     ]
    }
   ],
   "source": [
    "POD_PATH = \"/shared/3/projects/benlitterer/podcastData/podcastIndex/floydSubset.csv\"\n",
    "podDf = pd.read_csv(POD_PATH, index_col=0).drop(columns=[\"Unnamed: 0\"]).rename(columns={\"url\":\"rssUrl\", \"description\":\"podDescription\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "506791c9-725a-47d3-b8cc-27005d78d3e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "LOG_PATH = \"/home/blitt/projects/podcasts/mergeTransMetadata/logs/mergeTransMetadata/Jan2_2024.txt\"\n",
    "logFile = open(LOG_PATH, \"w\") \n",
    "logFile.write(f\"shape of RSS scraped metadata: {metaDf.shape}\\n\")\n",
    "logFile.write(f\"shape of podcastIndex metadata: {podDf.shape}\\n\")\n",
    "logFile.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "51c34ab4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2115280, 41)\n",
      "(2085463, 41)\n"
     ]
    }
   ],
   "source": [
    "print(podDf.shape)\n",
    "print(podDf.dropna(subset=[\"link\"]).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "91608a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#merge the metadata onto the podcast episode data \n",
    "merged = pd.merge(metaDf, podDf, on=[\"rssUrl\"], how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de4e70af-944e-4d65-a6cb-cbd273d88f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "logFile.write(f\"shape of data merged on rssUrl: {merged.shape}\\n\") \n",
    "logFile.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a2a35f2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(65715102, 53)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged.dropna(subset=[\"epTitle\"]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2f2b4922",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(metaDf) == len(merged) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "5bf2959c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parseTime(inStr, inFormat):\n",
    "    try: \n",
    "        return datetime.datetime.strptime(inStr, inFormat)\n",
    "\n",
    "        return pd.to_datetime(inStr, format=inFormat).tz_localize(None)\n",
    "    except Exception as E:\n",
    "        #print(E)\n",
    "        return np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "195206c6-9fc7-4c34-9999-c1fd48267dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "samp = merged.head(1000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "39871a0d-519f-4292-9a08-8e7487e3ef02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Timestamp('2023-05-01 08:00:00')"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.to_datetime(testStr, format=\"%a, %d %b %Y %H:%M:%S %z\").tz_localize(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "1be5aa4e-4ca3-4de8-abf7-632ae58e9990",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19.5"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(65 * 18) / 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "9c01a345-7598-488b-8a0e-c8b30a4e5188",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2867304/348279712.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  samp['date_formats_with_offset'] = samp[\"pubDate\"].apply(parseTime, inFormat=\"%a, %d %b %Y %H:%M:%S %z\")\n",
      "/tmp/ipykernel_2867304/348279712.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  samp[\"date_formats_without_offset\"] = samp[\"pubDate\"].apply(parseTime, inFormat=\"%a, %d %b %Y %H:%M:%S %Z\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19.354294776916504\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2867304/348279712.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  samp[\"cleanDates\"] = samp['date_formats_with_offset'].fillna(samp[\"date_formats_without_offset\"])\n"
     ]
    }
   ],
   "source": [
    "sTime = time.time()\n",
    "samp['date_formats_with_offset'] = samp[\"pubDate\"].apply(parseTime, inFormat=\"%a, %d %b %Y %H:%M:%S %z\")\n",
    "samp[\"date_formats_without_offset\"] = samp[\"pubDate\"].apply(parseTime, inFormat=\"%a, %d %b %Y %H:%M:%S %Z\") \n",
    "samp[\"cleanDates\"] = samp['date_formats_with_offset'].fillna(samp[\"date_formats_without_offset\"])\n",
    "print(time.time() - sTime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "8583e4fa-0ded-4c75-a0bb-8d9de6c6325b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get just the english first to save time\n",
    "merged = merged.dropna(subset=[\"language\"])\n",
    "logFile.write(f\"shape of data without NA's in language column: {merged.shape}\\n\") \n",
    "\n",
    "merged = merged[merged[\"language\"].str.lower().str.contains('en')]\n",
    "logFile.write(f\"shape of data with only English: {merged.shape}\\n\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "4f89127d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean date using only one format, leaving NaTs for unmatched format \n",
    "# tz_localize(None) means that our times are local to the places that they originated \n",
    "merged['date_formats_with_offset'] = merged[\"pubDate\"].apply(parseTime, inFormat=\"%a, %d %b %Y %H:%M:%S %z\")\n",
    "merged[\"date_formats_without_offset\"] = merged[\"pubDate\"].apply(parseTime, inFormat=\"%a, %d %b %Y %H:%M:%S %Z\") \n",
    "merged[\"cleanDates\"] = merged['date_formats_with_offset'].fillna(merged[\"date_formats_without_offset\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1eb30fd-9d06-4ab8-af6a-fdcb4f6f7bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "logFile.write(f\"finished date formatting\\n\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "3b3e459e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "naDates = sum(merged[\"cleanDates\"].isna())\n",
    "logFile.write(f\"# of date NA values: {naDates}\\n\")\n",
    "logFile.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "e30d7bb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "52"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#other cleaning things to do... \n",
    "#add transcript path \n",
    "#get function to give transcript file path of urls\n",
    "spec=importlib.util.spec_from_file_location(\"getURLstorageLocation\",\"/home/blitt/projects/podcasts/mergeTransMetadata/getURLstorageLocation.py\")\n",
    "foo = importlib.util.module_from_spec(spec)\n",
    "spec.loader.exec_module(foo)\n",
    "\n",
    "#get path to transcripts for each url \n",
    "#we will have to add META_PATH later, since we could be sending output to different plcaes \n",
    "META_PATH= \"\"\n",
    "merged = merged.dropna(subset=[\"enclosure\"])\n",
    "logFile.write(f\"shape of data after removing NA urls: {merged.shape}\\n\") \n",
    "merged[\"potentialOutPath\"] = merged[\"enclosure\"].apply(foo.getUrlTranscriptPath, args=[META_PATH])\n",
    "logFile.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "2c33146a",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged = merged.dropna(subset=[\"cleanDates\"]) \n",
    "logFile.write(f\"shape after removing cleanDate NA values: {merged.shape}\\n\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "bc127973",
   "metadata": {},
   "outputs": [],
   "source": [
    "#we started with 300k na's and ended at ~700k \n",
    "#so we have ~ 400k na values out of ~27 million rows "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "210f9c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged = merged.drop(columns=['date_formats_with_offset', 'date_formats_without_offset'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "320e53c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#merged.head(300)[\"cleanDates\"].apply(lambda x: x.replace(tzinfo=None))  > datetime.datetime.strptime(\"03-04-2023\", \"%m-%d-%Y\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffc240d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#merged = merged.rename(columns={\"description_x\":\"epDescription\", \"description_y\":\"podDescription\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48d9fae1-e88c-48ef-be5e-03648f93a3d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged[\"cleanDatesLoc\"] = pd.to_datetime(merged[\"cleanDates\"], utc=True, errors=\"coerce\")\n",
    "logFile.write(f\"forcing clean dates to UTC format (no offset assumes +00:00)\\n\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c95cf8a-912e-42f9-ba50-00533b8cae06",
   "metadata": {},
   "outputs": [],
   "source": [
    "logFile.write(f\"writing merged file to disk\\n\") \n",
    "merged.to_csv(\"/shared/3/projects/benlitterer/podcastData/processed/mergedMetaDataClean.csv\", quoting=csv.QUOTE_NONNUMERIC)\n",
    "logFile.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "61761b94-4719-48da-9c35-c5faa2e49dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#before cutoff\n",
    "firstCutoff = pd.to_datetime(\"05-03-2020\", format=\"%m-%d-%Y\", utc=True)\n",
    "lastCutoff = pd.to_datetime(\"05-09-2020\", format=\"%m-%d-%Y\", utc=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "89d262e4-e661-4868-828a-79da83a54e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "#merged.sample(30)[[\"pubDate\", \"cleanDates\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "f1429507",
   "metadata": {},
   "outputs": [],
   "source": [
    "#punch in here\n",
    "beforeFMonth = merged[(merged[\"cleanDatesLoc\"] >= firstCutoff) & (merged[\"cleanDatesLoc\"] <= lastCutoff)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "bf5be842-302c-457b-97c4-e8379338eaf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#punch in here \n",
    "#logFile.write(f\"dates for beforeFMonth are [05-03-2020, 05-09-2020], inclusive\\n\") \n",
    "#logFile.write(f\"writing week before Floyd Month file to disk\\n\") \n",
    "#logFile.flush()\n",
    "beforeFMonth.to_csv(\"/shared/3/projects/benlitterer/podcastData/processed/beforeFloydMonth/beforeFMonth.tsv\", quoting=csv.QUOTE_NONNUMERIC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "928ebf57",
   "metadata": {},
   "outputs": [],
   "source": [
    "#two weeks before was Monday, May 11, 2020\n",
    "#two weeks after was Monday, June 8, 2020\n",
    "logFile.write(f\"dates for Floyd Month are [05-11-2020, 06-08-2020], inclusive\\n\") \n",
    "logFile.write(f\"writing Floyd Month file to disk\\n\") \n",
    "logFile.flush()\n",
    "\n",
    "firstCutoff = pd.to_datetime(\"05-11-2020\", format=\"%m-%d-%Y\", utc=True)\n",
    "lastCutoff = pd.to_datetime(\"06-08-2020\", format=\"%m-%d-%Y\", utc=True)\n",
    "\n",
    "#OLD Line that was here \n",
    "#floydMonths = merged[(merged[\"date_format_same_final\"] >= \"05-11-2020\") & (merged[\"date_format_same_final\"] <= \"06-08-2020\")]\n",
    "\n",
    "floydMonths = merged[(merged[\"cleanDatesLoc\"] >= firstCutoff) & (merged[\"cleanDatesLoc\"] <= lastCutoff)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "c7c02fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "floydMonths.to_csv(\"/shared/3/projects/benlitterer/podcastData/processed/floydMonth/floydMonthEnSHORT.csv\", quoting=csv.QUOTE_NONNUMERIC)\n",
    "logFile.write(\"finished\") \n",
    "logFile.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

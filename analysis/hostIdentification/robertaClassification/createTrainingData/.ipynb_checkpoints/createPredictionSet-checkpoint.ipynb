{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a1725ec3-c97f-47ea-a23a-ab5517bf0bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "930b2b12-9fe5-47ad-9b9d-0e4e55d62a47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-02-15 17:36:28,446] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "import torch \n",
    "import transformers\n",
    "from transformers import PreTrainedTokenizer\n",
    "from transformers import RobertaTokenizer, RobertaConfig, RobertaModel, RobertaForTokenClassification\n",
    "#from sentence_transformers import SentenceTransformer, SentencesDataset, InputExample, losses, util\n",
    "from datasets import Dataset\n",
    "import pandas as pd\n",
    "from transformers.optimization import get_linear_schedule_with_warmup\n",
    "#import matplotlib.pyplot as plt \n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import AutoTokenizer, AutoModel, PreTrainedTokenizerFast, RobertaTokenizerFast\n",
    "#from torch.nn import CosineEmbeddingLoss\n",
    "import random\n",
    "from random import sample \n",
    "#from torch.nn import CosineEmbeddingLoss\n",
    "from torch import nn\n",
    "#Build up to SBERT model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "493210d3-035c-4a7b-8b7d-390ec3fe38ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from sklearn.metrics import f1_score, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from torch.utils.data import DataLoader,random_split,SubsetRandomSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9f59b73a-138a-4e87-b52d-76bbe6a32c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json(\"/shared/3/projects/benlitterer/podcastData/processed/floydMonth/floydMonthData.jsonl\", orient=\"records\", lines=True)\n",
    "\n",
    "#TODO: go back and make sure there's no data leakage \n",
    "#for now, get some data to check out the model with \n",
    "\n",
    "#fullSample = df.sample(1000, random_state=20) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "01381e83-fa7d-486b-9c12-3a34cb8ddae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sample 1000 podcasts (not episodes) \n",
    "random.seed(10)\n",
    "podSample = random.sample(list(df[\"rssUrl\"].unique()), 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "2c3a9f53-5ca7-4eee-98c9-bb067ee62349",
   "metadata": {},
   "outputs": [],
   "source": [
    "fullSample = df[df[\"rssUrl\"].isin(podSample)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "18ede44d-e59d-44ca-9505-a4a523cfaafd",
   "metadata": {},
   "outputs": [],
   "source": [
    "fullSample.to_json(\"/shared/3/projects/benlitterer/podcastData/hostIdentification/prediction2_15/1000sample.jsonl\", orient=\"records\", lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "e7f50d32-65b1-4e7c-8b9a-88daf5be2c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "fullSample = pd.read_json(\"/shared/3/projects/benlitterer/podcastData/hostIdentification/prediction2_15/1000sample.jsonl\", orient=\"records\", lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "937ba2a0-7bbe-40a2-a4aa-31f540b51723",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(480570, 69)\n",
      "(463934, 69)\n"
     ]
    }
   ],
   "source": [
    "#hmmm why don't we have entities for some of these? \n",
    "print(df.dropna(subset=[\"500ent\", \"500start\", '500end', '500type']).shape) \n",
    "\n",
    "print(df.dropna(subset=['DescEnt', 'DescStart', 'DescEnd',\n",
    "       'DescType']).shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "3e09dc7b-ade1-4494-83ef-6417d8c0edf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = df.dropna(subset=['500ent','500start', '500end', '500type', 'DescEnt', 'DescStart', 'DescEnd','DescType', 'first500'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "0a667fa3-41ce-4f64-ba43-8238cea2a32a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fullSample = fullSample[['potentialOutPath', 'wCount', 'rssUrl', 'epTitle', 'title', 'podDescription', '500ent','500start', '500end', '500type', 'DescEnt', 'DescStart', 'DescEnd',\n",
    "       'DescType', 'first500']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "9502410a-e9a4-47bf-a7d7-f5c5dd9f8c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = fullSample.dropna(subset=['500ent','500start', '500end', '500type', 'DescEnt', 'DescStart', 'DescEnd',\n",
    "       'DescType', 'first500'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "d91991a2-13b1-4bef-a4ba-2f8d3d51c653",
   "metadata": {},
   "outputs": [],
   "source": [
    "BEFORE_BUFF = 30\n",
    "AFTER_BUFF = 30\n",
    "\n",
    "def snippetsHelper(entStart, entEnd, snippet): \n",
    "    beforeWords = snippet[0:entStart].split(\" \")\n",
    "    if len(beforeWords) >= BEFORE_BUFF: \n",
    "        buffStart = \" \".join(beforeWords[-BEFORE_BUFF:]) \n",
    "    else: \n",
    "        buffStart = \" \".join(beforeWords) \n",
    "\n",
    "\n",
    "    afterWords = snippet[entEnd:len(snippet)].split(\" \")\n",
    "    if len(afterWords) >= AFTER_BUFF: \n",
    "        buffEnd = \" \".join(afterWords[:AFTER_BUFF]) \n",
    "    else: \n",
    "        buffEnd = \" \".join(afterWords) \n",
    "    return [buffStart, snippet[entStart:entEnd], buffEnd]\n",
    "\n",
    "def getSnippets(row): \n",
    "    \n",
    "    descText = row[\"podDescription\"]\n",
    "    descStarts = row[\"DescStart\"]\n",
    "    descEnds = row[\"DescEnd\"]\n",
    "    \n",
    "    transcriptText = row[\"first500\"]\n",
    "    transStarts = row[\"500start\"]\n",
    "    transEnds = row[\"500end\"]\n",
    "    outData = []\n",
    "    \n",
    "    if transcriptText != None and transcriptText == transcriptText: \n",
    "        for i in range(len(transStarts)): \n",
    "            transStart = transStarts[i]\n",
    "            transEnd = transEnds[i]\n",
    "\n",
    "            buffStart, ent, buffEnd = snippetsHelper(transStart, transEnd, transcriptText)\n",
    "            outData.append(buffStart + ent + buffEnd) \n",
    "    return outData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "32d39c60-bf76-4849-be0b-e2226d476c5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-250-0cbd21188bbd>:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  sample[\"transcriptSnippets\"] = sample.apply(getSnippets, axis=1)\n"
     ]
    }
   ],
   "source": [
    "sample[\"transcriptSnippets\"] = sample.apply(getSnippets, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "41a03f0f-c8c8-4053-8897-6027123a2312",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['potentialOutPath', 'wCount', 'rssUrl', 'epTitle', 'title',\n",
       "       'podDescription', '500ent', '500start', '500end', '500type', 'DescEnt',\n",
       "       'DescStart', 'DescEnd', 'DescType', 'first500', 'transcriptSnippets'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "ee4a20c7-bff9-493b-9df5-b7f9ca193ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "deviceNum = 1\n",
    "device = torch.device(\"cuda:\" + str(deviceNum) if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "45151c7b-a709-4d1e-a2f9-96eec7a237c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load in model for host detection\n",
    "transcriptDf = sample[['potentialOutPath', 'wCount', 'rssUrl', 'epTitle', 'title',\n",
    "       'podDescription', 'first500', '500ent', '500start', '500end', '500type', 'transcriptSnippets']].explode(['500ent', '500start', '500end', '500type', 'transcriptSnippets'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "d76b02ff-7468-4460-a062-16d011155409",
   "metadata": {},
   "outputs": [],
   "source": [
    "transcriptDf = transcriptDf[transcriptDf[\"500type\"] == \"PERSON\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "2935aace-41e6-48f5-afc3-bb953b326988",
   "metadata": {},
   "outputs": [],
   "source": [
    "transcriptDf = transcriptDf[transcriptDf[\"500ent\"].apply(lambda x: len(x.split())) == 2] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "e3bb9499-34d4-4a73-8e1f-531b0488b2a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[MUSIC] Hi\" and welcome to the accidental intellectual\" a podcast where we talk to people working in health-related fields and get to know the human behind the expert. I\\'m Lee Prop. Joined today by Ariana Simone. Hi listeners. In today\\'s episode\" we sat down with Diana Burtjel. Diana is a doctoral student in developmental psychology and education at the Ontario'"
      ]
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transcriptDf.iloc[0][\"transcriptSnippets\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "id": "197148c1-821b-4e3c-ae72-8019d62cab42",
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocess the transcript data \n",
    "#transcriptDf[\"snippetStart\"] = \n",
    "#.lower().find(x[\"500ent\"].lower())\n",
    "transcriptDf[\"snippetStart\"] = transcriptDf.apply(lambda x: x[\"transcriptSnippets\"].lower().find(x[\"500ent\"].lower()), axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "1314a836-afca-4c9a-96c9-c79d75cb95ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4953, 13)\n"
     ]
    }
   ],
   "source": [
    "print(transcriptDf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "abdc518a-5b21-49b4-8ab7-39b21042bdc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4953, 13)\n"
     ]
    }
   ],
   "source": [
    "transcriptDf = transcriptDf[transcriptDf[\"snippetStart\"] != -1]\n",
    "#valDf = valDf[valDf[\"snippetStart\"] != -1]\n",
    "print(transcriptDf.shape)\n",
    "\n",
    "#get where we start, and then add the length of the entity \n",
    "transcriptDf[\"snippetEnd\"] = transcriptDf[\"snippetStart\"] + transcriptDf[\"500end\"] - transcriptDf[\"500start\"]\n",
    "#valDf[\"snippetEnd\"] = valDf[\"snippetStart\"] + valDf[\"500end\"] - valDf[\"500start\"]\n",
    "\n",
    "def extractEnt(inRow): \n",
    "    return inRow[\"transcriptSnippets\"][inRow[\"snippetStart\"]:inRow[\"snippetEnd\"]]\n",
    "\n",
    "#just for sanity checking whether our inferred start/end indices are correct \n",
    "#looks good!\n",
    "transcriptDf[\"extractedEnt\"] = transcriptDf.apply(extractEnt, axis=1)\n",
    "#valDf[\"extractedEnt\"] = valDf.apply(extractEnt, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "id": "244fd376-4d7e-4f12-a16e-69bafaba4283",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing\n",
    "tokenizer = RobertaTokenizerFast.from_pretrained('roberta-base', max_length=512, padding=\"max_length\", truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "id": "84fe9358-0a84-46f5-a714-46f5cceba092",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized = []\n",
    "for snip in transcriptDf[\"transcriptSnippets\"]: \n",
    "    tokenized.append(tokenizer(snip, padding = \"max_length\", truncation=True, return_offsets_mapping=True))\n",
    "\n",
    "transcriptDf = pd.concat([transcriptDf.reset_index(drop=True), pd.DataFrame.from_records(tokenized)], axis=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "id": "0be79ad5-e1b9-4d7f-9fbf-ea099a0b3a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "#find the token indices which correspond to our entity \n",
    "def getTokenIndices(start, end, offsets):\n",
    "    \"\"\"\n",
    "    print(start) \n",
    "    print(end) \n",
    "    print(offsets[:20]) \n",
    "    \"\"\"\n",
    "\n",
    "    currIndices = []\n",
    "    for j, offset in enumerate(offsets): \n",
    "        offsetL, offsetR = offset\n",
    "        if offsetL >= start and offsetR <= end: \n",
    "            currIndices.append(j)\n",
    "\n",
    "    return currIndices\n",
    "\n",
    "#posTokens is useful with a BIO setup \n",
    "transcriptDf[\"posTokens\"] = transcriptDf.apply(lambda row: getTokenIndices(row[\"snippetStart\"], row[\"snippetEnd\"], row[\"offset_mapping\"]), axis=1)\n",
    "\n",
    "labList = []\n",
    "for i, row in transcriptDf.iterrows(): \n",
    "    tokCount = sum(row[\"attention_mask\"])\n",
    "    paddingLen = len(row[\"attention_mask\"]) - tokCount\n",
    "    \n",
    "    labels = ([0] * tokCount) + ([2] * paddingLen)\n",
    "    \n",
    "    for posIndex in row[\"posTokens\"]: \n",
    "        labels[posIndex] = 1\n",
    "    \n",
    "    labList.append(labels) \n",
    "\n",
    "transcriptDf[\"labels\"] = labList\n",
    "\n",
    "transcriptDf[\"entsTokenized\"] = transcriptDf.apply(lambda row: [tokenizer.decode(row[\"input_ids\"][i]) for i in row[\"posTokens\"]], axis=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "17b05a21-3164-4c0f-a3a3-7cccc3231469",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['potentialOutPath', 'wCount', 'rssUrl', 'epTitle', 'title',\n",
       "       'podDescription', 'first500', '500ent', '500start', '500end', '500type',\n",
       "       'transcriptSnippets', 'snippetStart', 'snippetEnd', 'extractedEnt',\n",
       "       'attention_mask', 'input_ids', 'offset_mapping', 'posTokens', 'labels',\n",
       "       'entsTokenized'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 263,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transcriptDf.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "id": "fa2b0d7a-12a9-49e6-b8f8-75f2ea685848",
   "metadata": {},
   "outputs": [],
   "source": [
    "transcriptDf = transcriptDf[transcriptDf[\"posTokens\"].apply(len) > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "id": "aae65b48-a22f-40bf-9e5e-4bb73d221888",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get an \"index\" column that just indexes the row of the dataframe we have \n",
    "transcriptDf = transcriptDf.reset_index(drop=True).reset_index()\n",
    "tokenIxList = list(transcriptDf[\"posTokens\"])\n",
    "\n",
    "dataset = Dataset.from_pandas(transcriptDf)\n",
    "dataset.set_format(type='torch', columns=[\"index\", \"transcriptSnippets\",\"input_ids\", \"attention_mask\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "id": "8dc4bfdc-16a0-4d91-9b10-411b82e1e58c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, ixList):\n",
    "        #def __init__(self):\n",
    "        super(Model,self).__init__()\n",
    "        self.model = RobertaModel.from_pretrained('roberta-base')\n",
    "        self.l1 = nn.Linear(768, 1)\n",
    "        self.sig = nn.Sigmoid()\n",
    "        self.ixList = ixList\n",
    "        \n",
    "    def mean_pooling(self, token_embeddings, attention_mask): \n",
    "        input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "        return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask, index): \n",
    "        \n",
    "        #encode sentence and get mean pooled sentence representation \n",
    "        output = self.model(input_ids, attention_mask=attention_mask)\n",
    "        \n",
    "        outIndices = [self.ixList[ix] for ix in index]\n",
    "        embeddingMeans = []\n",
    "        batchIter = 0\n",
    "        for batchIter in range(input_ids.shape[0]): \n",
    "            \n",
    "            #get the last layer of the model \n",
    "            hiddenStates = output[0]\n",
    "            \n",
    "            #get the embeddings corresponding to the entity we're interested in \n",
    "            tokStates = [hiddenStates[batchIter][tokIndex,:] for tokIndex in outIndices[batchIter]]\n",
    "            \n",
    "            #take the mean over all embeddings for an entity \n",
    "            embeddingMean = torch.stack(tokStates).mean(dim=0)\n",
    "            \n",
    "            #append this so we get the mean embedding for each \n",
    "            #training example in this batch \n",
    "            embeddingMeans.append(embeddingMean) \n",
    "            #embeddingMeans.append(hiddenStates[batchIter][outIndices[batchIter][0],:])\n",
    "            \n",
    "        embeddingMeans = torch.stack(embeddingMeans)\n",
    "        \"\"\"\n",
    "        working code just used this!\n",
    "        embeddingMeans = self.mean_pooling(output[0], attention_mask)\n",
    "        \"\"\"\n",
    "        probs = self.sig(self.l1(embeddingMeans)).squeeze()\n",
    "        \n",
    "        \n",
    "        return probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "id": "54b5c0a7-d9ab-4cc3-b97d-514f6dc08445",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Model(\n",
       "  (model): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): RobertaPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (l1): Linear(in_features=768, out_features=1, bias=True)\n",
       "  (sig): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 267,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load in trained model \n",
    "WEIGHTS_PATH=\"/shared/3/projects/benlitterer/podcastData/hostIdentification/robertaTokenClassifier/weights/feb15.weights\"\n",
    "model = Model(tokenIxList).to(device)\n",
    "model.load_state_dict(torch.load(WEIGHTS_PATH))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "id": "8a40722b-c40e-495a-8ff7-c8fc9b26aa07",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE=2\n",
    "loader = torch.utils.data.DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "id": "840533fb-433b-43fb-b83a-4317cb4f374f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "823614472a3742aba240a9dd0f5236e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2477 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "TypeError",
     "evalue": "'float' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[269], line 16\u001b[0m\n\u001b[1;32m     13\u001b[0m preds \u001b[38;5;241m=\u001b[39m (probs \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m.5\u001b[39m)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mint\u001b[39m)\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[1;32m     14\u001b[0m probs \u001b[38;5;241m=\u001b[39m probs\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[0;32m---> 16\u001b[0m allProbs \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m probs \n\u001b[1;32m     17\u001b[0m allPreds \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m preds\n\u001b[1;32m     18\u001b[0m indices \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mindex\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mtolist()\n",
      "\u001b[0;31mTypeError\u001b[0m: 'float' object is not iterable"
     ]
    }
   ],
   "source": [
    "allProbs = []\n",
    "allPreds = []\n",
    "indices = []\n",
    "i = 0 \n",
    "for batch in tqdm(loader): \n",
    "    \n",
    "    input_ids = batch[\"input_ids\"].to(device) \n",
    "    attention_mask = batch[\"attention_mask\"].to(device) \n",
    "    index = batch[\"index\"]\n",
    " \n",
    "    #gt = batch[\"groundTruth\"].to(device).to(torch.float32)\n",
    "    probs = model(input_ids, attention_mask, index) #.to(torch.float32)\n",
    "    preds = (probs > .5).to(int).cpu().tolist()\n",
    "    probs = probs.cpu().tolist()\n",
    "    \n",
    "    allProbs += probs \n",
    "    allPreds += preds\n",
    "    indices += batch[\"index\"].cpu().tolist()\n",
    "    if i % 5 == 0: \n",
    "        torch.cuda.empty_cache()\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "id": "c29b7ebf-a64a-4f36-9278-ac2a5cb3c858",
   "metadata": {},
   "outputs": [],
   "source": [
    "predsDf = pd.DataFrame({\"index\":indices, \"prob\":allProbs, \"pred\":allPreds})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "id": "b00826b2-980f-4672-a25b-055aeb091117",
   "metadata": {},
   "outputs": [],
   "source": [
    "outDf = transcriptDf.iloc[:len(allPreds)]\n",
    "#outDf = pd.concat([outDf, predsDf], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "id": "e7adbc2e-1316-4fad-969f-f6b212135b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "outDf = pd.merge(outDf, predsDf, on=\"index\", how=\"inner\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "id": "ad7dcadf-3b5e-4131-966f-0fcf97bf10cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['index', 'potentialOutPath', 'wCount', 'rssUrl', 'epTitle', 'title',\n",
       "       'podDescription', 'first500', '500ent', '500start', '500end', '500type',\n",
       "       'transcriptSnippets', 'snippetStart', 'snippetEnd', 'extractedEnt',\n",
       "       'attention_mask', 'input_ids', 'offset_mapping', 'posTokens', 'labels',\n",
       "       'entsTokenized', 'prob', 'pred'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 273,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outDf.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "id": "0e3ed167-95d5-48f9-8db0-7c1912c8aa8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>first500</th>\n",
       "      <th>extractedEnt</th>\n",
       "      <th>entsTokenized</th>\n",
       "      <th>500type</th>\n",
       "      <th>prob</th>\n",
       "      <th>pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1108</th>\n",
       "      <td>content (upbeat music) - Hey everybody\" welcom...</td>\n",
       "      <td>Oliver Reed</td>\n",
       "      <td>[ Oliver,  Reed]</td>\n",
       "      <td>PERSON</td>\n",
       "      <td>0.001699</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1327</th>\n",
       "      <td>content Hello everyone. My name is Brendan Moo...</td>\n",
       "      <td>Darth Vader</td>\n",
       "      <td>[ Darth,  Vader]</td>\n",
       "      <td>PERSON</td>\n",
       "      <td>0.001743</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1934</th>\n",
       "      <td>content [Music] This is gonna be the heading c...</td>\n",
       "      <td>Billy Allen</td>\n",
       "      <td>[ Billy,  Allen]</td>\n",
       "      <td>PERSON</td>\n",
       "      <td>0.001759</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4144</th>\n",
       "      <td>content [Music] Welcome to Film Fight Club\" I'...</td>\n",
       "      <td>Jimmy Stewart</td>\n",
       "      <td>[ Jimmy,  Stewart]</td>\n",
       "      <td>PERSON</td>\n",
       "      <td>0.001795</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2639</th>\n",
       "      <td>content I'm Red Robinson and these are the Leg...</td>\n",
       "      <td>Carl Perry</td>\n",
       "      <td>[ Carl,  Perry]</td>\n",
       "      <td>PERSON</td>\n",
       "      <td>0.001805</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2981</th>\n",
       "      <td>content [MUSIC] Hello and welcome. This is Per...</td>\n",
       "      <td>Nick Thompson</td>\n",
       "      <td>[ Nick,  Thompson]</td>\n",
       "      <td>PERSON</td>\n",
       "      <td>0.998208</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1078</th>\n",
       "      <td>content Hi ladies and gentlemen\" we're just go...</td>\n",
       "      <td>Al Ronald</td>\n",
       "      <td>[ Al,  Ronald]</td>\n",
       "      <td>PERSON</td>\n",
       "      <td>0.998225</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2632</th>\n",
       "      <td>content I'm Red Robinson and these are the Leg...</td>\n",
       "      <td>Red Robinson</td>\n",
       "      <td>[ Red,  Robinson]</td>\n",
       "      <td>PERSON</td>\n",
       "      <td>0.998248</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>547</th>\n",
       "      <td>content Hello and welcome to this week's episo...</td>\n",
       "      <td>Nathan Dean</td>\n",
       "      <td>[ Nathan,  Dean]</td>\n",
       "      <td>PERSON</td>\n",
       "      <td>0.998250</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2984</th>\n",
       "      <td>content [MUSIC] Hello and welcome. This is Per...</td>\n",
       "      <td>Nick Thompson</td>\n",
       "      <td>[ Nick,  Thompson]</td>\n",
       "      <td>PERSON</td>\n",
       "      <td>0.998266</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4952 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               first500   extractedEnt  \\\n",
       "1108  content (upbeat music) - Hey everybody\" welcom...    Oliver Reed   \n",
       "1327  content Hello everyone. My name is Brendan Moo...    Darth Vader   \n",
       "1934  content [Music] This is gonna be the heading c...    Billy Allen   \n",
       "4144  content [Music] Welcome to Film Fight Club\" I'...  Jimmy Stewart   \n",
       "2639  content I'm Red Robinson and these are the Leg...     Carl Perry   \n",
       "...                                                 ...            ...   \n",
       "2981  content [MUSIC] Hello and welcome. This is Per...  Nick Thompson   \n",
       "1078  content Hi ladies and gentlemen\" we're just go...      Al Ronald   \n",
       "2632  content I'm Red Robinson and these are the Leg...   Red Robinson   \n",
       "547   content Hello and welcome to this week's episo...    Nathan Dean   \n",
       "2984  content [MUSIC] Hello and welcome. This is Per...  Nick Thompson   \n",
       "\n",
       "           entsTokenized 500type      prob  pred  \n",
       "1108    [ Oliver,  Reed]  PERSON  0.001699     0  \n",
       "1327    [ Darth,  Vader]  PERSON  0.001743     0  \n",
       "1934    [ Billy,  Allen]  PERSON  0.001759     0  \n",
       "4144  [ Jimmy,  Stewart]  PERSON  0.001795     0  \n",
       "2639     [ Carl,  Perry]  PERSON  0.001805     0  \n",
       "...                  ...     ...       ...   ...  \n",
       "2981  [ Nick,  Thompson]  PERSON  0.998208     1  \n",
       "1078      [ Al,  Ronald]  PERSON  0.998225     1  \n",
       "2632   [ Red,  Robinson]  PERSON  0.998248     1  \n",
       "547     [ Nathan,  Dean]  PERSON  0.998250     1  \n",
       "2984  [ Nick,  Thompson]  PERSON  0.998266     1  \n",
       "\n",
       "[4952 rows x 6 columns]"
      ]
     },
     "execution_count": 274,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outDf[[\"first500\", \"extractedEnt\", \"entsTokenized\", \"500type\", \"prob\", \"pred\"]].sort_values(\"prob\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "id": "96b04281-3cfc-446a-9eb0-f3ed341de307",
   "metadata": {},
   "outputs": [],
   "source": [
    "outDf.to_csv(\"/shared/3/projects/benlitterer/podcastData/hostIdentification/prediction2_15/transcriptEntPreds.csv\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "id": "8f312257-69b5-40be-bd64-759307dfa489",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>potentialOutPath</th>\n",
       "      <th>wCount</th>\n",
       "      <th>rssUrl</th>\n",
       "      <th>title</th>\n",
       "      <th>podDescription</th>\n",
       "      <th>first500</th>\n",
       "      <th>500ent</th>\n",
       "      <th>prob</th>\n",
       "      <th>pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/shared/3/projects/benlitterer/podcastData/pro...</td>\n",
       "      <td>7813</td>\n",
       "      <td>https://feeds.buzzsprout.com/591025.rss</td>\n",
       "      <td>Accidental Intellectual</td>\n",
       "      <td>Scientists are human, too. So are therapists a...</td>\n",
       "      <td>content [MUSIC] Hi\" and welcome to the acciden...</td>\n",
       "      <td>Lee Prop</td>\n",
       "      <td>0.998070</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/shared/3/projects/benlitterer/podcastData/pro...</td>\n",
       "      <td>7813</td>\n",
       "      <td>https://feeds.buzzsprout.com/591025.rss</td>\n",
       "      <td>Accidental Intellectual</td>\n",
       "      <td>Scientists are human, too. So are therapists a...</td>\n",
       "      <td>content [MUSIC] Hi\" and welcome to the acciden...</td>\n",
       "      <td>Ariana Simone</td>\n",
       "      <td>0.996384</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/shared/3/projects/benlitterer/podcastData/pro...</td>\n",
       "      <td>7813</td>\n",
       "      <td>https://feeds.buzzsprout.com/591025.rss</td>\n",
       "      <td>Accidental Intellectual</td>\n",
       "      <td>Scientists are human, too. So are therapists a...</td>\n",
       "      <td>content [MUSIC] Hi\" and welcome to the acciden...</td>\n",
       "      <td>Diana Burtjel</td>\n",
       "      <td>0.010395</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/shared/3/projects/benlitterer/podcastData/pro...</td>\n",
       "      <td>10573</td>\n",
       "      <td>https://www.spreaker.com/show/4355441/episodes...</td>\n",
       "      <td>Fuse Control Podcast</td>\n",
       "      <td>Welcome to The Fuse Control Podcast hosted by ...</td>\n",
       "      <td>content (upbeat music) - Well\" welcome to Show...</td>\n",
       "      <td>Gary V.</td>\n",
       "      <td>0.056136</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/shared/3/projects/benlitterer/podcastData/pro...</td>\n",
       "      <td>10573</td>\n",
       "      <td>https://www.spreaker.com/show/4355441/episodes...</td>\n",
       "      <td>Fuse Control Podcast</td>\n",
       "      <td>Welcome to The Fuse Control Podcast hosted by ...</td>\n",
       "      <td>content (upbeat music) - Well\" welcome to Show...</td>\n",
       "      <td>Ashley Graham</td>\n",
       "      <td>0.002572</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    potentialOutPath  wCount  \\\n",
       "0  /shared/3/projects/benlitterer/podcastData/pro...    7813   \n",
       "1  /shared/3/projects/benlitterer/podcastData/pro...    7813   \n",
       "2  /shared/3/projects/benlitterer/podcastData/pro...    7813   \n",
       "3  /shared/3/projects/benlitterer/podcastData/pro...   10573   \n",
       "4  /shared/3/projects/benlitterer/podcastData/pro...   10573   \n",
       "\n",
       "                                              rssUrl                    title  \\\n",
       "0            https://feeds.buzzsprout.com/591025.rss  Accidental Intellectual   \n",
       "1            https://feeds.buzzsprout.com/591025.rss  Accidental Intellectual   \n",
       "2            https://feeds.buzzsprout.com/591025.rss  Accidental Intellectual   \n",
       "3  https://www.spreaker.com/show/4355441/episodes...     Fuse Control Podcast   \n",
       "4  https://www.spreaker.com/show/4355441/episodes...     Fuse Control Podcast   \n",
       "\n",
       "                                      podDescription  \\\n",
       "0  Scientists are human, too. So are therapists a...   \n",
       "1  Scientists are human, too. So are therapists a...   \n",
       "2  Scientists are human, too. So are therapists a...   \n",
       "3  Welcome to The Fuse Control Podcast hosted by ...   \n",
       "4  Welcome to The Fuse Control Podcast hosted by ...   \n",
       "\n",
       "                                            first500         500ent      prob  \\\n",
       "0  content [MUSIC] Hi\" and welcome to the acciden...       Lee Prop  0.998070   \n",
       "1  content [MUSIC] Hi\" and welcome to the acciden...  Ariana Simone  0.996384   \n",
       "2  content [MUSIC] Hi\" and welcome to the acciden...  Diana Burtjel  0.010395   \n",
       "3  content (upbeat music) - Well\" welcome to Show...        Gary V.  0.056136   \n",
       "4  content (upbeat music) - Well\" welcome to Show...  Ashley Graham  0.002572   \n",
       "\n",
       "   pred  \n",
       "0     1  \n",
       "1     1  \n",
       "2     0  \n",
       "3     0  \n",
       "4     0  "
      ]
     },
     "execution_count": 276,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outDf[[\"potentialOutPath\", \"wCount\", \"rssUrl\", \"title\", \"podDescription\", \"first500\", \"500ent\", \"prob\", \"pred\"]].head() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "id": "f36d49e9-e65b-44e8-aea9-c4dacc435b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "groupedOut = outDf[[\"potentialOutPath\", \"wCount\", \"rssUrl\", \"title\", \"podDescription\", \"first500\", \"500ent\", \"prob\", \"pred\"]].groupby(by=[\"rssUrl\"]).agg(list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "id": "4ee824f2-706e-4a80-ad8a-be2ab9be5db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "groupedOut[\"wCount\"] = groupedOut[\"wCount\"].apply(lambda x: x[0]) \n",
    "groupedOut[\"podDescription\"] = groupedOut[\"podDescription\"].apply(lambda x: x[0]) \n",
    "\n",
    "groupedOut[\"title\"] = groupedOut[\"title\"].apply(lambda x: set(x))\n",
    "groupedOut[\"potentialOutPath\"] = groupedOut[\"potentialOutPath\"].apply(lambda x: set(x))\n",
    "groupedOut[\"first500\"] = groupedOut[\"first500\"].apply(lambda x: set(x))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "id": "5357cc7e-e0d2-43f5-8c22-99486d37dcb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sepPosNegPreds(row):\n",
    "    ents = row[\"500ent\"]\n",
    "    preds = row[\"pred\"]\n",
    "    \n",
    "    pos = [ents[i] for i in range(len(ents)) if preds[i] == 1]\n",
    "    neg = [ents[i] for i in range(len(ents)) if preds[i] == 0]\n",
    "    \n",
    "    return [pos, neg]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "id": "237736e1-f8b8-4a94-9795-c1b50bda9c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "posNegPreds = groupedOut.apply(sepPosNegPreds, axis=1)\n",
    "\n",
    "posPreds = [item[0] for item in posNegPreds]\n",
    "negPreds = [item[1] for item in posNegPreds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "id": "44ef7b14-3d77-4d23-a7d0-d6257612767d",
   "metadata": {},
   "outputs": [],
   "source": [
    "groupedOut[\"hostPredictions\"] = posPreds\n",
    "groupedOut[\"nonHostPredictions\"] = negPreds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "id": "88404149-8316-4bed-be82-f4b156a1a3a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "groupedOut.to_csv(\"/shared/3/projects/benlitterer/podcastData/hostIdentification/prediction2_15/transcriptGroupedPreds.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02b60d39-5009-4e51-8820-e8fc3da89849",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

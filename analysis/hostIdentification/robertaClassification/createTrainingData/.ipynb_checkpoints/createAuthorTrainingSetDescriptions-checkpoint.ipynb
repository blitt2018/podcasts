{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d5f77837-0895-4e4e-bffc-f80c944d4f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e8d7db21-280d-4e3d-8605-eeb5f0fe5a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataframe with the entities extracted from the introduction of each podcast \n",
    "entDf = pd.read_csv(\"/shared/3/projects/benlitterer/podcastData/NER/podDescriptions/floydMonthNEs.tsv\", sep=\"\\t\", names=[\"potentialOutPath\", \"ent\", \"start\", \"end\", \"type\"])\n",
    "entDf = entDf[entDf[\"type\"] == \"PERSON\"] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "164c80fa-43d2-4c1c-93aa-2b064c4d1e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "entDf[\"potentialOutPath\"] = \"/shared/3/projects/benlitterer/podcastData/prosodyMerged/floydMonth\" + entDf[\"potentialOutPath\"] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "546e4893-17f5-44c4-bfc3-4003ed6195b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-5-ff88e9f54b40>:2: DtypeWarning: Columns (6,7,8,12,20,49) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  metaDf = pd.read_csv(\"/shared/3/projects/benlitterer/podcastData/processed/floydMonth/floydMonthEn.csv\")\n"
     ]
    }
   ],
   "source": [
    "#metadata (this has author field)  \n",
    "metaDf = pd.read_csv(\"/shared/3/projects/benlitterer/podcastData/processed/floydMonth/floydMonthEn.csv\")\n",
    "metaDf[\"potentialOutPath\"] = \"/shared/3/projects/benlitterer/podcastData/prosodyMerged/floydMonth\" + metaDf[\"potentialOutPath\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5284b1f9-3572-4e83-b5f7-634e7608d9a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#merges on only the transcripts we actually have \n",
    "#merge metadata, transcript beginnings, and entities\n",
    "df = pd.merge(entDf, metaDf[[\"podDescription\", \"itunesAuthor\",\"rssUrl\", \"potentialOutPath\"]], on=\"potentialOutPath\", how=\"inner\")\n",
    "df = df.dropna() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3c0a654c-0489-4844-a61c-54694560bc11",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sort the dataframe so that within each episode (potentialOutPath)\n",
    "#we have the starts of the entities in ascending order\n",
    "df = df.sort_values([\"potentialOutPath\", \"start\"]) \n",
    "\n",
    "#we want to figure out where to create our snippets \n",
    "df = df.groupby(\"potentialOutPath\").agg(list) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1f0692d7-6838-4797-a0f3-0b9c53ab1716",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "353245it [01:02, 5654.86it/s]\n"
     ]
    }
   ],
   "source": [
    "BEFORE_BUFF = 10 \n",
    "AFTER_BUFF = 10\n",
    "\n",
    "#go through and create snippets of text \n",
    "entSnippets = []\n",
    "for i, row in tqdm(df.iterrows()): \n",
    "    prevEntEnd = 0 \n",
    "    currEntSnippets = []\n",
    "    for j in range(len(row[\"start\"])): \n",
    "        snippet = row[\"podDescription\"][j]\n",
    "        entStart = row[\"start\"][j]\n",
    "        entEnd = row[\"end\"][j]\n",
    "        \n",
    "        #get position that is BUFFER words before and after entity\n",
    "        beforeWords = snippet[0:entStart].split(\" \")\n",
    "        \n",
    "        #only if we have enough words before to get entire buffer \n",
    "        if len(beforeWords) >= BEFORE_BUFF: \n",
    "            #get the snippet before \n",
    "            buffStart = entStart - len(\" \".join(beforeWords[-BEFORE_BUFF:])) -1\n",
    "        else: \n",
    "            buffStart = entStart - len(\" \".join(beforeWords)) -1\n",
    "        \n",
    "        #get position that is BUFFER words before and after entity\n",
    "        afterWords = snippet[entEnd:len(snippet)].split(\" \")\n",
    "        \n",
    "        #only if we have enough words after to get entire buffer\n",
    "        if len(afterWords) >= AFTER_BUFF: \n",
    "            #get the snippet after\n",
    "            buffEnd = entEnd + len(\" \".join(afterWords[:AFTER_BUFF])) + 1\n",
    "        else:  \n",
    "            buffEnd = entEnd + len(\" \".join(afterWords)) + 1\n",
    "        \n",
    "        \"\"\"\n",
    "        testing\n",
    "        print(row[\"ent\"][j])\n",
    "        print(snippet[entStart:buffEnd])\n",
    "        print(\"---------------\") \n",
    "        \"\"\"\n",
    "        \n",
    "        \"\"\"\n",
    "        right now this is set up so that we don't include entities before \n",
    "        \n",
    "        \"\"\"\n",
    "        snippetLeft = max(prevEntEnd, buffStart) \n",
    "        \n",
    "        #if we have a next entity to look ahead to, see where it starts\n",
    "        #if it starts before where our buffer will end, stop when we hit that next entity \n",
    "        if j + 1 < len(row[\"start\"]):\n",
    "            snippetRight = min(row[\"start\"][j + 1], buffEnd)\n",
    "        else: \n",
    "            snippetRight = buffEnd\n",
    "            \n",
    "        currEntSnippets.append(snippet[snippetLeft: snippetRight])\n",
    "        prevEntEnd = entEnd \n",
    "    entSnippets.append(currEntSnippets) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7634cb88-f686-45b9-ad78-13ce0e039668",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"entSnippets\"] = entSnippets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f8ad07c4-bade-4976-b066-3c4f0144d61a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#now that we've gotten the snippets we can explode back out\n",
    "df = df.explode(list(df.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "b9b2740c-1840-4e49-b15f-d12aeb84854f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Laura\n",
      " out on the latest episodes. You can also buy Laura a Chai Latte to express your thanks at https://www.buymeacoffee.com/soulcoach \n"
     ]
    }
   ],
   "source": [
    "pd.options.display.max_colwidth = 25\n",
    "currSample = df.sample(20)\n",
    "ix = 19\n",
    "print(list(currSample[\"ent\"])[ix])  \n",
    "print(list(currSample[\"entSnippets\"])[ix])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "f49dcbe0-6b6a-4602-8a1c-7f5c89013a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "#now we want to determine whether an entity is a ground truth label or not \n",
    "#we can do so by checking if it is contained in the itunes author field \n",
    "def entIsAuthor(inRow):\n",
    "    if inRow[\"ent\"] == inRow[\"ent\"]: \n",
    "        ent = inRow[\"ent\"] \n",
    "    else: \n",
    "        return False \n",
    "    if inRow[\"itunesAuthor\"] == inRow[\"itunesAuthor\"]: \n",
    "        auth = inRow[\"itunesAuthor\"]\n",
    "    else: \n",
    "        return False \n",
    "        \n",
    "    return ent.lower() in auth.lower()\n",
    "\n",
    "df[\"entInAuthor\"] = df.apply(lambda x: x[\"ent\"].lower() in x[\"itunesAuthor\"].lower(), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "94e1ee65-983a-4362-8f10-ef26ece614db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(994159, 9)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "97f06e31-51e7-43e0-af27-0ac0b2e5c30c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "353245"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(df.index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "b8613465-d593-48be-8621-42043e50409c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "148423"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "posClass = df[df[\"entInAuthor\"] == True]\n",
    "len(set(posClass.index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "3dcb9bf2-d8b7-4ceb-b940-0fd14949e834",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "frac. of podcasts for which entity exists 0.4201701368738411\n",
      "frac. of podcasts for which we have metadata 0.19789733333333334\n"
     ]
    }
   ],
   "source": [
    "#we can get decent ground truths for around 1 / 2 of the podcasts for which we have entities \n",
    "#\n",
    "print(f\"frac. of podcasts for which entity exists {148423 / 353245}\") \n",
    "print(f\"frac. of podcasts for which we have metadata {148423/750000}\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "046c1324-61bb-4151-9bc3-1c4c8ec02bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=[\"podDescription\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "db262b70-ee79-4c36-b2c2-866980dfbe30",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"snippetLen\"] = df[\"entSnippets\"].apply(lambda x: len(x.split())) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "bbc5df9e-bcf3-4454-a9c9-97dc6b6ddb8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(994159, 9)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "61d91acf-015e-41f3-abe4-e92a0f0de25e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#but need to figure out why this is happening..\n",
    "df = df[df[\"snippetLen\"] > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "d45cf2e7-15a9-4668-b60b-397aa3d62f5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "posClass = df[df[\"entInAuthor\"] == True] \n",
    "negClass = df[df[\"entInAuthor\"] == False].sample(len(posClass)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "d81c1d85-644b-4461-bda5-ea76a2b1c01a",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainData = pd.concat([posClass, negClass], axis=0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "f2d0fc36-fade-4965-872f-a531a1df5304",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainData.to_csv(\"/shared/3/projects/benlitterer/podcastData/hostIdentification/itunesGTsubsetDescriptions.tsv\", sep=\"\\t\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bc6fd6b-b246-4b33-9346-143e593eaa98",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

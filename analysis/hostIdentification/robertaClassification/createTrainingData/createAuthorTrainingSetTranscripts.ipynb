{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "d5f77837-0895-4e4e-bffc-f80c944d4f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e8d7db21-280d-4e3d-8605-eeb5f0fe5a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataframe with the entities extracted from the introduction of each podcast \n",
    "entDf = pd.read_csv(\"/shared/3/projects/benlitterer/podcastData/NER/podIntroductions/floydMonth500wordNEs.csv\", names=[\"potentialOutPath\", \"ent\", \"start\", \"end\", \"type\"])\n",
    "entDf = entDf[entDf[\"type\"] == \"PERSON\"] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "546e4893-17f5-44c4-bfc3-4003ed6195b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#metadata (this has author field)  \n",
    "metaDf = pd.read_csv(\"/shared/3/projects/benlitterer/podcastData/processed/floydMonth/floydMonthEn.csv\")\n",
    "metaDf[\"potentialOutPath\"] = \"/shared/3/projects/benlitterer/podcastData/prosodyMerged/floydMonth\" + metaDf[\"potentialOutPath\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5dc286d8-9b71-49a3-808f-1a25e69d2127",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataframe with the first bit (500 words) of each podcast \n",
    "introDf = pd.read_csv(\"/shared/3/projects/benlitterer/podcastData/NER/floydIntroductions.tsv\", sep=\"\\t\", names=[\"potentialOutPath\", \"first500\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5284b1f9-3572-4e83-b5f7-634e7608d9a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#merges on only the transcripts we actually have \n",
    "#merge metadata, transcript beginnings, and entities\n",
    "df = pd.merge(entDf, introDf, on=\"potentialOutPath\", how=\"inner\") \n",
    "df = pd.merge(df, metaDf[[\"itunesAuthor\",\"rssUrl\", \"potentialOutPath\"]], on=\"potentialOutPath\", how=\"inner\") \\\n",
    "\n",
    "df = df.dropna() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "3c0a654c-0489-4844-a61c-54694560bc11",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sort the dataframe so that within each episode (potentialOutPath)\n",
    "#we have the starts of the entities in ascending order\n",
    "df = df.sort_values([\"potentialOutPath\", \"start\"]) \n",
    "\n",
    "#we want to figure out where to create our snippets \n",
    "df = df.groupby(\"potentialOutPath\").agg(list) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "1f0692d7-6838-4797-a0f3-0b9c53ab1716",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "69225it [01:44, 660.42it/s] \n"
     ]
    }
   ],
   "source": [
    "BEFORE_BUFF = 10 \n",
    "AFTER_BUFF = 10\n",
    "\n",
    "#go through and create snippets of text \n",
    "entSnippets = []\n",
    "for i, row in tqdm(df.iterrows()): \n",
    "    prevEntEnd = 0 \n",
    "    currEntSnippets = []\n",
    "    for j in range(len(row[\"start\"])): \n",
    "        snippet = row[\"first500\"][j]\n",
    "        entStart = row[\"start\"][j]\n",
    "        entEnd = row[\"end\"][j]\n",
    "        \n",
    "        #get position that is BUFFER words before and after entity\n",
    "        beforeWords = snippet[0:entStart].split(\" \")\n",
    "        \n",
    "        #only if we have enough words before to get entire buffer \n",
    "        if len(beforeWords) >= BEFORE_BUFF: \n",
    "            #get the snippet before \n",
    "            buffStart = entStart - len(\" \".join(beforeWords[-BEFORE_BUFF:])) -1\n",
    "        else: \n",
    "            buffStart = entStart - len(\" \".join(beforeWords)) -1\n",
    "        \n",
    "        #get position that is BUFFER words before and after entity\n",
    "        afterWords = snippet[entEnd:len(snippet)].split(\" \")\n",
    "        \n",
    "        #only if we have enough words after to get entire buffer\n",
    "        if len(afterWords) >= AFTER_BUFF: \n",
    "            #get the snippet after\n",
    "            buffEnd = entEnd + len(\" \".join(afterWords[:AFTER_BUFF])) + 1\n",
    "        else:  \n",
    "            buffEnd = entEnd + len(\" \".join(afterWords)) + 1\n",
    "        \n",
    "        \"\"\"\n",
    "        testing\n",
    "        print(row[\"ent\"][j])\n",
    "        print(snippet[entStart:buffEnd])\n",
    "        print(\"---------------\") \n",
    "        \"\"\"\n",
    "        \n",
    "        \"\"\"\n",
    "        right now this is set up so that we don't include entities before \n",
    "        \n",
    "        \"\"\"\n",
    "        snippetLeft = max(prevEntEnd, buffStart) \n",
    "        \n",
    "        #if we have a next entity to look ahead to, see where it starts\n",
    "        #if it starts before where our buffer will end, stop when we hit that next entity \n",
    "        if j + 1 < len(row[\"start\"]):\n",
    "            snippetRight = min(row[\"start\"][j + 1], buffEnd)\n",
    "        else: \n",
    "            snippetRight = buffEnd\n",
    "            \n",
    "        currEntSnippets.append(snippet[snippetLeft: snippetRight])\n",
    "        prevEntEnd = entEnd \n",
    "    entSnippets.append(currEntSnippets) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "7634cb88-f686-45b9-ad78-13ce0e039668",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"entSnippets\"] = entSnippets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "f8ad07c4-bade-4976-b066-3c4f0144d61a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#now that we've gotten the snippets we can explode back out\n",
    "df = df.explode(list(df.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "f49dcbe0-6b6a-4602-8a1c-7f5c89013a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "#now we want to determine whether an entity is a ground truth label or not \n",
    "#we can do so by checking if it is contained in the itunes author field \n",
    "def entIsAuthor(inRow):\n",
    "    if inRow[\"ent\"] == inRow[\"ent\"]: \n",
    "        ent = inRow[\"ent\"] \n",
    "    else: \n",
    "        return False \n",
    "    if inRow[\"itunesAuthor\"] == inRow[\"itunesAuthor\"]: \n",
    "        auth = inRow[\"itunesAuthor\"]\n",
    "    else: \n",
    "        return False \n",
    "        \n",
    "    return ent.lower() in auth.lower()\n",
    "\n",
    "df[\"entInAuthor\"] = df.apply(lambda x: x[\"ent\"].lower() in x[\"itunesAuthor\"].lower(), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "046c1324-61bb-4151-9bc3-1c4c8ec02bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=[\"first500\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "d45cf2e7-15a9-4668-b60b-397aa3d62f5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "posClass = df[df[\"entInAuthor\"] == True] \n",
    "negClass = df[df[\"entInAuthor\"] == False].sample(len(posClass) * 4) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "d81c1d85-644b-4461-bda5-ea76a2b1c01a",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainData = pd.concat([posClass, negClass], axis=0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "f2d0fc36-fade-4965-872f-a531a1df5304",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainData.to_csv(\"/shared/3/projects/benlitterer/podcastData/hostIdentification/itunesGTsubset.tsv\", sep=\"\\t\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bc6fd6b-b246-4b33-9346-143e593eaa98",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
